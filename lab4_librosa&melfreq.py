# -*- coding: utf-8 -*-
"""Lab4_librosa&melfreq.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-F_Vb4Ohx9bHetLrzMf9ZBOKsydw4xqs
"""

from google.colab import drive
drive.mount('/content/drive')

from glob import glob
import librosa
import pandas as pd
import numpy as np
from scipy.spatial.distance import minkowski
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
import pywt
from matplotlib import pyplot as plt
from sklearn.metrics import classification_report
from sklearn.model_selection import learning_curve

def get_features(audios,labels):
  features_total=[]

  for i,x in enumerate(audios):  # Iterate over audio files and their corresponding labels
    y,sr=librosa.load(x)        # Load y-audio file and obtain the sampling rate (sr)
    D=librosa.stft(y)           # Compute Short-Time Fourier Transform (STFT) and convert to dB scale
    S_db=librosa.amplitude_to_db(np.abs(D),ref=np.max)
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13) # Extract Mel-frequency cepstral coefficients (MFCCs) #taking 13 features

    features=[]
    for f in mfccs:    # Calculate mean of each MFCC feature column
      feature=np.mean(f)
      features.append(feature)
    features.append(labels[i]) # Append the corresponding label to the feature vector
    features_total.append(features) # Append the feature vector to the list

  features_total=np.array(features_total) # Convert the list of feature vectors to a NumPy array

  features_total=pd.DataFrame(features_total,columns=["F1","F2","F3","F4","F5","F6","F7","F8","F9","F10","F11","F12","F13","Label"]) # Create a DataFrame with column names for features and label
  return features_total

POS ='/content/drive/MyDrive/aidataset/gunshot'
NEG = '/content/drive/MyDrive/aidataset/nongunshot'

pos = glob(POS + '/*.wav')
neg = glob(NEG + '/*.wav')

labels_1=[1]*len(pos) #list of files with label 1 - gunshot
labels_0=[0]*len(neg) #list of files with label 0 - non gunshot
fgun=get_features(pos,labels_1)
fnongun=get_features(neg,labels_0)

print(fgun)
print(fnongun)

# Repeat positive examples to match the number of negative examples
repeat_count = len(fnongun)// len(fgun)
remainder = len(fnongun) % len(fgun)

#nfgun = fgun
#for _ in range(repeat_count - 1):
    #nfgun = nfgun.concatenate(fgun)
nfgun = pd.concat([fgun] * repeat_count + [fgun.iloc[:remainder]]) #converting the files into a dataframe using pandas
data = pd.concat([fnongun, nfgun])
x = data.iloc[:, :-1] #feature vectors
y = data.iloc[:-1] #labels

#A1 Evaluate intraclass spread and interclass distances
def A1(fgun,fnongun):
  gun_mean = fgun.mean(axis=0)
  nongun_mean = fnongun.mean(axis=0)
  gun_std = fgun.std(axis=0)
  nongun_std = fnongun.std(axis=0)
  class_dist = np.linalg.norm(gun_mean - nongun_mean)
  return gun_mean,nongun_mean,gun_std,nongun_std,class_dist

gun_mean,nongun_mean,gun_std,nongun_std,class_dist=A1(nfgun,fnongun)
print("Gun Mean:\n", gun_mean)
print("Non-Gun Mean:\n", nongun_mean)
print("Gun Standard Deviation:\n", gun_std)
print("Non-Gun Standard Deviation:\n", nongun_std)
print("Interclass Distance:\n", class_dist)

#A2
def a2(data,index):
  feature_index = index
  feature_data = data.iloc[:, feature_index]

  # Calculate histogram
  hist, bins = np.histogram(feature_data, bins=20, density=True)

  # Plot histogram
  plt.bar(bins[:-1], hist, width=(bins[1]-bins[0]))
  plt.title('Density Pattern of Feature {}'.format(feature_index))
  plt.xlabel('Feature Values')
  plt.ylabel('Density')
  plt.show()

  # Calculate mean and variance
  mean_value = feature_data.mean()
  variance_value = feature_data.var()
  return mean_value,variance_value

index=1
mean_value,variance_value=a2(data,index)
print("Mean of Feature {}: {}".format(index, mean_value))
print("Variance of Feature {}: {}".format(index, variance_value))

#A3
#feature vector 1 & 2
vector1 = fgun.iloc[0, :-1]
vector2 = fnongun.iloc[0, :-1]

r_values = range(1, 11)
#minkowski distance
distances = [minkowski(vector1, vector2, p) for p in r_values]

# Plot the distance
plt.plot(r_values, distances)
plt.xlabel('r')
plt.ylabel('Distance')
plt.title('Minkowski Distance')
plt.show()

#A4
X = data.iloc[:, :-1]  # Features
y = data.iloc[:, -1]   # Labels

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) #splitting into training and testing data
#A5 , A6 , A7
#Train kNN classifier (k = 3)
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X_train, y_train)
#Test accuracy of kNN
accuracy = neigh.score(X_test, y_test) #k is increasing, accuracy is decreasing
print("Accuracy:", accuracy)
#Use predict() function to study prediction behavior
ypredict = neigh.predict(X_test)
print("Predictions:", ypredict)

#A8 Compare kNN with NN classifier (k = 1 to 11)
accuracies = []
for k in range(1, 12):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    accuracy = knn.score(X_test, y_test)
    accuracies.append(accuracy)
plt.plot(range(1, 12), accuracies)
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.title('Accuracy vs k for kNN')
plt.show()

#A9 Evaluate confusion matrix and other performance metrics like precision, recall, F1 score
conf_matrix = confusion_matrix(y_test, ypredict)
precision = precision_score(y_test, ypredict)
recall = recall_score(y_test, ypredict)
f1 = f1_score(y_test, ypredict)
print("Confusion Matrix:\n", conf_matrix)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

report = classification_report(y_test, ypredict)
print("\n",report)

# Define the kNN classifier
neigh = KNeighborsClassifier(n_neighbors=3)

# Generate learning curves
train_sizes, train_scores, test_scores = learning_curve(neigh, X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5)

# Compute mean and standard deviation of train and test scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot learning curves
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training accuracy')
plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')
plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='Validation accuracy')
plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')

# Add labels, title, and legend
plt.xlabel('Number of training samples')
plt.ylabel('Accuracy')
plt.title('Learning Curves')
plt.legend(loc='lower right')
plt.grid()
plt.show()

X_project = data.iloc[:, :-1]
y_project = data.iloc[:, -1]

# Split the data into training and testing sets
X_train_project, X_test_project, y_train_project, y_test_project = train_test_split(X_project, y_project, test_size=0.3)
#3
# Visualization of Training Data
# Assuming you choose two features for visualization
plt.scatter(X_train_project.iloc[:, 0], X_train_project.iloc[:, 1], c=y_train_project)
plt.title('Project Training Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

#4
# Classification with kNN
# Using k = 3 as an example
knn_project = KNeighborsClassifier(n_neighbors=3)
knn_project.fit(X_train_project, y_train_project)
predictions_y = knn_project.predict(X_test_project)

# Plotting the test data output with predicted class colors
colors_project = ['blue' if label == 0 else 'red' for label in predictions_y]
plt.scatter(X_test_project.iloc[:, 0], X_test_project.iloc[:, 1], c=colors_project, s=1)
plt.title('Project Test Data Output with Predicted Class Colors (k=3)')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

#5
# Repeat for Various Values of k
accuracies_project = []
for k in range(1, 12):
    knn_project = KNeighborsClassifier(n_neighbors=k)
    knn_project.fit(X_train_project, y_train_project)
    accuracy_project = knn_project.score(X_test_project, y_test_project)
    accuracies_project.append(accuracy_project)

plt.plot(range(1, 12), accuracies_project)
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.title('Accuracy vs k for kNN (Project Data)')
plt.show()

from sklearn.model_selection import RandomizedSearchCV

# Define the parameter grid for hyperparameter tuning
param_grid_project = {'n_neighbors': range(1, 21)}

# Initialize the kNN classifier
knn_project = KNeighborsClassifier()

#7
# Perform Randomized Search CV
random_search_project = RandomizedSearchCV(knn_project, param_distributions=param_grid_project, n_iter=10)
random_search_project.fit(X_train_project, y_train_project)

# Get the best parameters
print("Best parameters:", random_search_project.best_params_)